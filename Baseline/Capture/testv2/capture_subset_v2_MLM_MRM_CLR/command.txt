Namespace(CLR=True, ITM=False, MLM=True, MRM=True, baseline=False, bert_model='bert-base-chinese', caption_path='../../item_valid_info.jsonl', config_file='./config/capture.json', distributed=False, do_lower_case=True, fp16=False, freeze=-1, from_pretrained='./pytorch_model_8.bin', gradient_accumulation_steps=1, img_weight=1, label_list_file=None, learning_rate=0.0001, lmdb_file=None, local_rank=-1, loss_scale=0, max_seq_length=36, no_cuda=False, num_train_epochs=10.0, num_workers=3, on_memory=False, output_dir='./testv2', predict_feature=True, save_name='capture_subset_v2_MLM_MRM_CLR', seed=42, start_epoch=0, test_lmdb_file=None, train_batch_size=64, train_file='data/conceptual_caption/training', train_lmdb_file=None, use_chuncks=0, validation_file='data/conceptual_caption/validation', warmup_proportion=0.1, without_coattention=False)


{
  "attention_probs_dropout_prob": 0.1,
  "bi_attention_type": 1,
  "bi_hidden_size": 1024,
  "bi_intermediate_size": 1024,
  "bi_num_attention_heads": 8,
  "fast_mode": false,
  "fixed_t_layer": 0,
  "fixed_v_layer": 0,
  "fusion_method": "mul",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "in_batch_pairs": false,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "intra_gate": false,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pooling_method": "mul",
  "predict_feature": false,
  "t_biattention_id": [
    6,
    7,
    8,
    9,
    10,
    11
  ],
  "type_vocab_size": 2,
  "v_attention_probs_dropout_prob": 0.1,
  "v_biattention_id": [
    6,
    7,
    8,
    9,
    10,
    11
  ],
  "v_feature_size": 2048,
  "v_hidden_act": "gelu",
  "v_hidden_dropout_prob": 0.1,
  "v_hidden_size": 768,
  "v_initializer_range": 0.02,
  "v_intermediate_size": 1024,
  "v_num_attention_heads": 8,
  "v_num_hidden_layers": 12,
  "v_target_size": 2048,
  "vocab_size": 21128,
  "with_coattention": true
}

